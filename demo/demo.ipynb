{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af3f25b",
   "metadata": {},
   "source": [
    "# User guide for `ddsimca` package\n",
    "\n",
    "This guide is mainly based on [DD-SIMCA tutorial paper](https://doi.org/10.1002/cem.3556) and most of the code below reproduces outcomes and figures shown in the paper. Therefore it is higly recommended to download and read the paper first (it is freely available for everyone) and then come back to this document. \n",
    "\n",
    "Before you start, make sure that you have installed the `ddsimca` package. If not, just uncomment and run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5157344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ddsimca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd95d50",
   "metadata": {},
   "source": [
    "It will automatically install all necessary packages, including `prcv`, which implements Procrustes cross-validation, to be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802eff3c",
   "metadata": {},
   "source": [
    "## Training DD-SIMCA model and detection of outliers\n",
    "\n",
    "[Download](https://mda.tools/ddsimca/Oregano.zip) zip archive with the Oregano dataset used in the tutorial.  It consists of several CSV files, just unzip them all to the same folder where you have this document. \n",
    "\n",
    "The following code loads training set from a CSV file and shows first five rows and five columns of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9f7d9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data_train = pd.read_csv(\"Target_Train.csv\", index_col=0)\n",
    "\n",
    "print(data_train.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc8a32",
   "metadata": {},
   "source": [
    "As you can see, the data rows have object labels (hence we used `index_col=0` when loaded the data). The first column of the dataset contains class labels. For the training set it should only contain target class label (in our case `\"Oregano\"`), if there are more labels or no labels at all you will get an error when trying to create a model. The rest of the data frame consists of NIR spectra, already preprocessed.\n",
    "\n",
    "Now we ready to train DD-SIMCA model and look at summary info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1edeef",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from ddsimca import ddsimca\n",
    "m = ddsimca(data_train, ncomp = 10)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa87c15",
   "metadata": {},
   "source": [
    "As you can see, the value `10` is the total number of components to use in the model. The optimal number can be discovered and set later, by default it is the same as the total number.\n",
    "\n",
    "Also by default the data values were mean centered but not standardized. This can be changed by providing extra arguments, check help for `ddsimca()` method for more details.\n",
    "\n",
    "You can visualize the number of degrees of freedom and the eigenvalues vs. number of PCs using plots (eigenvalues can also be shown log transformed, check help for the method):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b65325",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "m.plotDoF(ax1, dof = \"Nh\")\n",
    "m.plotDoF(ax1, dof = \"Nq\")\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "m.plotEigenvals(ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b4196",
   "metadata": {},
   "source": [
    "As well as show plots with the PCA loadings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "m.plotLoadings(ax1, comp = (1, 2), type = \"p\")\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "m.plotLoadings(ax2, comp = (1,), type = \"l\", color = \"blue\")\n",
    "m.plotLoadings(ax2, comp = (2,), type = \"l\", color = \"red\")\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"Wavenumber, cm-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72599644",
   "metadata": {},
   "source": [
    "Here parameter `type` tells how to show the loadings values, `\"p\"` stands for points (scatter plot), and `\"l\"` stands for lines. The parameter `comp` should be a tuple with two values for scatter plot and with one value for line plot, as it is shown above.\n",
    "\n",
    "The model object does not have any results, it only contains values and statistics needed for applying this model to any dataset (e.g. loadings, values for centering and scaling, parameters of distance distribution, etc.). In order to get the results, you need to apply this model to a dataset. Here is how to do it for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbe16a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_train_c = m.predict(data_train)\n",
    "r_train_c.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52ed53",
   "metadata": {},
   "source": [
    "As you can see, most of the parameters, like significance level for extremes, `alpha`, and outliers, `gamma`, type of estimators for distance limits (`lim_type`) are set to default values (`0.05`, `0.01`, and `\"classic\"` correspondingly). If you want to change any of them, simply provide the proper values as arguments of the method `predict()`.\n",
    "\n",
    "For example, let's create the result object using robust estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a911c8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_train_r = m.predict(data_train, lim_type = \"robust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff5e72",
   "metadata": {},
   "source": [
    "Now let's check the acceptance plot for both results object (obtained using classic and robust estimators) in order to find any outliers. The figure below shows plots similar to (A) and (B) from Figure 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe3dd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "r_train_c.plotAcceptance(ax1, ncomp = 2, show_labels = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "r_train_r.plotAcceptance(ax2, ncomp = 2, show_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f7cf9",
   "metadata": {},
   "source": [
    "We can also show these two plots using log transformed coordinates, like in Figure 3 of the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "r_train_c.plotAcceptance(ax1, ncomp = 2, do_log = True, show_labels = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "r_train_r.plotAcceptance(ax2, ncomp = 2, do_log = True, show_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97e418",
   "metadata": {},
   "source": [
    "Apparently, as described in the paper, we need to remove the sample `Drg12` first, and then `Drg13`. Let's do this step by step and reproduce plots (C) and (D) of Figure 2. Note, that every time we remove an outlier we need to re-train the model.\n",
    "\n",
    "First remove `Drg12` and reproduce the plot (C):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_new = data_train.drop(\"Drg12\")\n",
    "\n",
    "m_new = ddsimca(data_train_new, 10)\n",
    "r_train_new = m_new.predict(data_train_new, lim_type = \"robust\")\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "r_train_new.plotAcceptance(ax, ncomp = 2, show_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166b87a",
   "metadata": {},
   "source": [
    "Now let's remove `Drg13` and reproduce plot (D) of the Figure 2. Note, that in this case we get back to classic estimators, as from the paper we know that there are no more outliers in the data. Otherwise it can be a good idea to use robust estimators again as well as to check this plot for different number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb42f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_train_final = data_train_new.drop(\"Drg13\")\n",
    "\n",
    "m_final = ddsimca(data_train_final, 10)\n",
    "r_train_final = m_final.predict(data_train_final)\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "r_train_final.plotAcceptance(ax, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db43b7",
   "metadata": {},
   "source": [
    "Finally, here are sensitivity and extremes plots made for the training set, similar to what is shown in Figure 4 in the paper. There is one difference, in the paper we used A = 20 components in the model and here we used 10 (to make summary outcomes shorter). Therefore sensitivity plot below is shown for 10 first components only.\n",
    "\n",
    "Here we use a versatile method `plotFoM()` which can show a plot for any of the five figures of merit: sensitivity (`\"sens\"`), specificity (`\"spec\"`), selectivity (`\"sel\"`), accuracy (`\"acc\"`) and  efficiency (`\"eff\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea081b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "r_train_final.plotFoM(ax1, fom = \"sens\", show_ci = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "r_train_final.plotExtremes(ax2, ncomp = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c42a3c",
   "metadata": {},
   "source": [
    "In case if FoM plot is made for sensitivity, you have a possibility to add 95% confidence interval computed based on the expected sensitivity (1 - alpha) and the number of objects in the dataset, it is shown as semi-transparent rectangle on the plot above. Similar to what is shown in the paper and what is implemented in the web-application.\n",
    "\n",
    "## Validation and optimization\n",
    "\n",
    "The best validation strategy is to use an independent validation set. However, if you want to keep it for the final testing of your model (or for fine tuning) you can employ [Procrustes cross-validation](https://github.com/svkucheryavski/pcv), PCV. PCV is a procedure for generation of validation set based on training set and cross-validation resampling. The code below does this based on PCA version of the method, implemented in package `prcv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prcv.methods import pcvpca\n",
    "\n",
    "# get matrix with predictors from the training set\n",
    "X_train = data_train_final.iloc[:, 1:].values\n",
    "\n",
    "# generate matrix with PV-set using 20 PCs, mean centering and cross-Validation\n",
    "# systematic splits (venetian blinds) to 4 segments\n",
    "X_pv = pcvpca(X_train, ncomp = 20, center = True, scale = False, cv = {\"type\": \"ven\", \"nseg\": 4})\n",
    "\n",
    "# create data frame from the generated data\n",
    "data_pv = pd.DataFrame(X_pv, index = data_train_final.index)\n",
    "data_pv.insert(0, \"Class\", data_train_final.Class)\n",
    "data_pv.columns = data_train_final.columns\n",
    "\n",
    "data_pv.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beceae44",
   "metadata": {},
   "source": [
    "Now let's apply the model to the PV-set and show a combined sensitivity plot for the training set and for the PV-set, hence reproducing plot (A) from Figure 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409e9bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_pv = m_final.predict(data_pv)\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "r_train_final.plotFoM(ax, fom = \"sens\", label = \"train\", show_ci = True)\n",
    "r_pv.plotFoM(ax, fom = \"sens\", label = \"pv\", color = \"tab:red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b2fc4",
   "metadata": {},
   "source": [
    "It looks like 3 components is optimal in this case. Let's now load the test set, which consists only of the target class members and apply the model to this set as well. Then we will show sensitivity plot for the test set, reproducing plot (B) of Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_target = pd.read_csv(\"Target_Test.csv\", index_col=0)\n",
    "\n",
    "r_test_target = m_final.predict(data_test_target)\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "r_test_target.plotFoM(ax, fom = \"sens\", show_ci = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cde10",
   "metadata": {},
   "source": [
    "Finally, you can also specify the optimal number of components for a model as well as for any result object. In this case, every time you make an acceptance plot (or do any other actions which depends on the number of components in the model) this value will be used as the default one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d928dd7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "m_final.select_ncomp(3)\n",
    "r_test_target.select_ncomp(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87b119",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Predictions can be made using data frames with or without reference class labels. In the first case the result object will contain all necessary figures of merits (sensitivity for members, specificity and selectivity for non-members, accuracy and efficiency if dataset contains both members and strangers). If reference classes are not provided, the model will just make predictions (accepted/rejected).\n",
    "\n",
    "Let's load dataset with reference classes (only non-target objects) and then remove column with class names, thus creating a data set without reference classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ade5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_nontarget = pd.read_csv(\"NonTarget_Non_Or.csv\", index_col = 0)\n",
    "data_new_nontarget = data_test_nontarget.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037407de",
   "metadata": {},
   "source": [
    "Let's check what is inside the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe3b70",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_test_nontarget.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137758",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_new_nontarget.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685c24e",
   "metadata": {},
   "source": [
    "Now let's apply the model to both sets (remember that they contain the same objects, but one has column with reference class labels and second one does not have thus column). Then check the acceptance plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a37853",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_nontarget = m_final.predict(data_test_nontarget)\n",
    "r_new_nontarget = m_final.predict(data_new_nontarget)\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "r_test_nontarget.plotAcceptance(ax1, do_log = True, show_labels = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "r_new_nontarget.plotAcceptance(ax2, do_log = True, show_labels = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e199e99",
   "metadata": {},
   "source": [
    "As you can see, in the first case the model indeed treated the objects as from non-target classes and shows corresponding roles (alien and external in this case) on the plot. While in the second case it splits samples to accepted (in) and rejected (out).\n",
    "\n",
    "Let's see how different the summary information is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_nontarget.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee184336",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_new_nontarget.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c04cf4",
   "metadata": {},
   "source": [
    "The main difference is that for the second dataset there are no columns with figures of merits, true negatives and false positives.\n",
    "\n",
    "Now let's load data which has reference classes and objects of both target and non-target classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3316709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_all = pd.read_csv(\"All_Test.csv\", index_col = 0)\n",
    "data_test_all.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2e081",
   "metadata": {},
   "source": [
    "There are several differences here. First of all, the acceptance plot can now be shown only for members, only for strangers, or for all samples (default option). In the latter case, the acceptance plot will color group object points by classes instead of roles until you change this by providing explicit value for parameter `show_set`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0492b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all = m_final.predict(data_test_all)\n",
    "\n",
    "plt.figure(figsize = (13, 4))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "r_test_all.plotAcceptance(ax1, do_log = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "r_test_all.plotAcceptance(ax2, do_log = True, show_set = \"strangers\")\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "r_test_all.plotAcceptance(ax3, do_log = True, show_set = \"members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d7fe",
   "metadata": {},
   "source": [
    "Also, in this case all figures of merit, including accuracy and efficiency, are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_all.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ffd22",
   "metadata": {},
   "source": [
    "And they can be plotted together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcc01f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "r_test_all.plotFoM(ax, fom = \"sens\")\n",
    "r_test_all.plotFoM(ax, fom = \"spec\")\n",
    "r_test_all.plotFoM(ax, fom = \"eff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d06aa9",
   "metadata": {},
   "source": [
    "## Extra plots, features, and details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a39bd7",
   "metadata": {},
   "source": [
    "It is also possible to show every distance separately for given number of components (this will work for any result objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 14))\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "r_test_all.plotDistance(ax1, ncomp = 2, distance = \"h\")\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "r_test_all.plotDistance(ax2, ncomp = 2, distance = \"q\")\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "r_test_all.plotDistance(ax3, ncomp = 2, distance = \"f\", show_crit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30caef4",
   "metadata": {},
   "source": [
    "As well as to show PCA scores plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "r_test_all.plotScores(ax1, comp = (1, 2), type = \"p\", show_labels = True)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "r_test_all.plotScores(ax2, comp = (1,), type = \"l\", color = \"blue\")\n",
    "r_test_all.plotScores(ax2, comp = (2,), type = \"l\", color = \"red\")\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698f36c",
   "metadata": {},
   "source": [
    "Similar to web-application you can also make plot with expected vs. observed alien objects and selectivity vs. sensitivity plot (if there are non-target class objects in the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 4))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "r_test_all.plotAliens(ax1)\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "r_test_all.plotSelectivity(ax2)\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 3)\n",
    "r_test_all.plotSelectivity(ax2, ncomp = 1, color = \"tab:blue\", label = \"A = 1\")\n",
    "r_test_all.plotSelectivity(ax2, ncomp = 3, color = \"tab:green\", label = \"A = 3\")\n",
    "r_test_all.plotSelectivity(ax2, ncomp = 5, color = \"tab:orange\", label = \"A = 5\")\n",
    "ax2.set_title(\"Selectivity\")\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac149d",
   "metadata": {},
   "source": [
    "Similar to distance, extremes, and acceptance plots, the plots above are made for the optimal number of components we pre-selected earlier (`A = 3`). You can change this by providing value using `ncomp` argument to the plotting methods.\n",
    "\n",
    "Any result object can be also converted to a data frame, where every object is represented by several columns, such as reference class (if provided), decision (in/out), role, as well as distance values. By default these values will be computed for optimal number of components, but you can force and provide which number of components to use for creating the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = r_test_all.as_df(ncomp = 2)\n",
    "rdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60ed3b",
   "metadata": {},
   "source": [
    "You can get access to all computed values and do whatever you want. For example in case of model, you can get loadings, vectors for centering and scaling, classic and robust parameters of distance distribution (for h, q and f-distances), etc. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c74ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings\n",
    "m_final.V[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fa4e1",
   "metadata": {},
   "source": [
    "The distance parameters are saved as dictionaries `hParams`, `qParams` and `fParams`. Each dictionary has two fields, `classic` and `robust`. Each field contains another tuple with scaling factors (e.g. `h0`) and number of degrees of freedom (e.g. `Nh`) computed for each component in the model. Here is an example for score distance parameters computed using classic estimates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c79d3b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "m_final.hParams[\"classic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a517272",
   "metadata": {},
   "source": [
    "Here is another example showing how to get `q0` and `Nq` values for robust estimator and 2 PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602efd3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "q0, Nq = m_final.qParams[\"robust\"]\n",
    "A = 2\n",
    "print(f\"A = {A}: q0 = {q0[A - 1]:.3f}, Nq = {Nq[A - 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6199197",
   "metadata": {},
   "source": [
    "In case of result object you can get all outcomes, including critical values, distances, calculations related to Type II error (if non-target class objects are provided) and many other things by getting access to data frame outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ea6ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.outcomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd52d4",
   "metadata": {},
   "source": [
    "You can also get access to matrices (`nrows` x `ncomp`) with h-, q- and f-distances, and matrix with decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e9ed6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.H[:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e97d7a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.Q[:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_all.F[:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9a301",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.D[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea82834",
   "metadata": {},
   "source": [
    "Here is for example decisions obtained for each object using A = 3 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bb102",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.D[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867322ff",
   "metadata": {},
   "source": [
    "As well as matrix with roles, which are coded by integer values: 0 for `regular`, 1 for `extreme`, 2 for `outlier`, 3 for `alien` and 4 for `external`. The first three are assigned to target class members and the last two â€” to objects from non-target classes and to unknown objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4588f20",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "r_test_all.R[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958479e",
   "metadata": {},
   "source": [
    "Here is a simple way to get the role names only for members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "r_names = np.array([\"regular\", \"extreme\", \"outlier\", \"alien\", \"external\"])\n",
    "r_names[r_test_all.R[:, 2]]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
